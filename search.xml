<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Algorithms for Learning To Rank Challenge</title>
      <link href="/2025/06/07/Algorithms-for-Learning-To-Rank-Challenge/"/>
      <url>/2025/06/07/Algorithms-for-Learning-To-Rank-Challenge/</url>
      
        <content type="html"><![CDATA[<p>之前参加推荐系统比赛的时候使用了xgboost作为排序模型，以NDCG作为模型训练的目标函数。之前没有理解为什么对成本函数做了二阶泰勒展开的xgboost可以使用对模型的打分不可微的NDCG为优化目标，这次把坑填上，顺带梳理了相关算法的发展脉络。</p><h3 id="preliminary-knowledge">Preliminary Knowledge</h3><p>排序是搜索，推荐，广告，社交网络系统等应用的核心技术之一，它需要对用户发起的请求返回的一组结果依据相关性排序。</p><p>排序算法可以分成两类：1.基于学习成对偏好的方法，比如RankNet，LambdaRank，2.使用回归或分类的方法直接预测相关性标签。对于第一种方法，排序算法的效果可以用对成对偏好的预测能力衡量，因此学习排序函数可以等价于学习预测成对偏好相对顺序的函数。对于第二种方法，模型可以完美预测相关性标签可以证明排序性能好，但是模型预测相关性标签的效果差却并不能反推排序能力弱。因此将排序问题转换为预测成对偏好问题的方法优于转换为分类或者回归问题，避免了学习一个不必要的困难问题。同时，第一种方法只需要成对的偏好关系，不需要具体的相关性标注，减少了数据标注的工作量和难度。</p><p>为了方便描述，接下来所有排序算法的讲解都以搜索场景为背景。</p><blockquote><p>Notation</p><ul><li>$ x ^n $: url的特征向量</li><li><span class="math inline"><em>U</em> ∈ ℝ</span>: url</li><li><span class="math inline"><em>f</em></span>: underlying model</li><li><spanclass="math inline"><em>s</em> = <em>f</em>(<em>x</em>)</span>:模型对url的打分</li><li>$ U_i ▷ U_j$: <spanclass="math inline"><em>U</em><sub><em>i</em></sub></span>的排名高于<spanclass="math inline"><em>U</em><sub><em>j</em></sub></span></li><li><span class="math inline"><em>σ</em> ∈ ℝ</span>:用于改变sigmoid形状的参数</li><li><span class="math inline"><em>C</em></span>: cost function</li><li><spanclass="math inline"><em>w</em><sub><em>k</em></sub> ∈ ℝ</span>：underlyingmodel的参数</li></ul></blockquote><h4 id="ranknet">RankNet</h4><p>RankNet的训练数据按照query划分，同一组内的url两两成对，RankNet使用url对为输入。RankNet将underlyingmodel对一对url的打分映射为一个概率： <span class="math display">$$P_{ij} = P(U_i ▷ U_j) = \frac{1} {1 + e^{-\sigma(s_i - s_j)}}\tag{1}$$</span> 然后使用交叉熵损失函数： <span class="math display">$$C = -{\overline{P} } _ {ij}\log{P_{ij} } - (1 -\overline{P}_{ij})\log{(1 - P_{ij})}\tag{2}$$</span> 对于一个给定query，令<spanclass="math inline"><em>S</em><sub><em>i</em><em>j</em></sub> ∈ 0, ±1</span>，1表示url<span class="math inline"><em>i</em></span>的相关性比url <spanclass="math inline"><em>j</em></span>高，0表示相关性相同，-1表示url<span class="math inline"><em>i</em></span>的相关性比url <spanclass="math inline"><em>j</em></span>低。则有： <spanclass="math display">$$\overline{P}_{ij} = \frac{1} {2} (1 + S_{ij})\tag{3}$$</span> 结合公式1，2，3： <span class="math display">$$C = \frac{1} {2}(1- S_{ij})\sigma(s_i - s_j) + \log{(1 + e ^{-\sigma(s_i- s_j)})}\tag{4}$$</span> 当<spanclass="math inline"><em>S</em><sub><em>i</em><em>j</em></sub> = 1</span>时：<spanclass="math display"><em>C</em> = log (1 + <em>e</em><sup>−<em>σ</em>(<em>s</em><sub><em>i</em></sub> − <em>s</em><sub><em>j</em></sub>)</sup>)</span>当<spanclass="math inline"><em>S</em><sub><em>i</em><em>j</em></sub> = −1</span>时：<spanclass="math display"><em>C</em> = log (1 + <em>e</em><sup>−<em>σ</em>(<em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>i</em></sub>)</sup>)</span>对<span class="math inline"><em>C</em></span>求偏导，显然具有对称性：<span class="math display">$$\frac{\partial{C} } {\partial{s_i} } = \sigma(\frac{1}{2} (1 - S_{ij}) -\frac{1} {1 + e^{\sigma(s_i - s_j)} }) = -\frac{\partial{C} }{\partial{s_j} }\tag{7}$$</span> 使用随机梯度下降法更新参数: <span class="math display">$$w_k \rightarrow w_k - \eta \frac{\partial{C} } {\partial{w_k} } = w_k -\eta( \frac{\partial{C} } {\partial{s_i} } \frac{\partial{s_i} }{\partial{w_k} } + \frac{\partial{C} } {\partial{s_j} }\frac{\partial{s_j} } {\partial{w_k} })\tag{8}$$</span></p><h4 id="factoring-ranknet">Factoring RankNet</h4><p>相比于RankNet使用随机梯度下降法，每接受一个样本对就更新一次参数，FactoringRankNet积累了每个url在所有包含它的样本对中的做出的贡献，再更新参数，属于小批量学习。<span class="math display">$$\frac{\partial{C} } {\partial{w_k} } = \frac{\partial{C} }{\partial{s_i}} \frac{\partial{s_i} } {\partial{w_k} } + \frac{\partial{C} }{\partial{s_j} } \frac{\partial{s_j} } {\partial{w_k} } =\sigma(\frac{1} {2}(1 - S_{ij}) - \frac{1} {1 + e ^{\sigma(s_i - s_j)}})(\frac{\partial{s_i} } {\partial{w_k} } - \frac{\partial{s_j} }{\partial{w_k} }) = \lambda_{ij}(\frac{\partial{s_i} } {\partial{w_k} }- \frac{\partial{s_j} } {\partial{w_k} })\tag{9}$$</span> 其中： <span class="math display">$$\lambda_{ij} = \sigma(\frac{1} {2}(1 - S_{ij}) - \frac{1} {1 + e^{\sigma(s_i - s_j)} }) = \frac{\partial{C} } {\partial{s_i} }\tag{10}$$</span> 令集合<span class="math inline"><em>I</em></span>为索引对<spanclass="math inline"><em>i</em>, <em>j</em></span>的集合，因为<spanclass="math inline"><em>i</em>, <em>j</em></span>只参与一次计算，可以规定集合<spanclass="math inline"><em>I</em></span>中的索引对都满足<spanclass="math inline"><em>U</em><sub><em>i</em></sub> ▷ <em>U</em><sub><em>j</em></sub></span>，则<spanclass="math inline"><em>S</em><sub><em>i</em><em>j</em></sub> = 1</span>。将所有样本对更新模型参数的贡献合并：<span class="math display">$$\delta{w_k} = -\eta \sum_{\{i, j\} \in I}(\lambda_{ij}\frac{\partial{s_i} } {\partial{w_k} } -\lambda_{ij}\frac{\partial{s_j} } {\partial{w_k} }) = -\eta\sum_i\lambda_i \frac{\partial{s_i} } {\partial{w_k} }\tag{11}$$</span> 其中： <spanclass="math display"><em>λ</em><sub><em>i</em></sub> = ∑<sub><em>j</em> : {<em>i</em>, <em>j</em>} ∈ <em>I</em></sub><em>λ</em><sub><em>i</em><em>j</em></sub> − ∑<sub><em>j</em> : {<em>j</em>, <em>i</em>} ∈ <em>I</em></sub><em>λ</em><sub><em>i</em><em>j</em></sub></span>可以看出，对于任意样本对，相关性更高的样本会得到向上推的作用力，相关性低的样本受到向下的作用力，两个作用力大小相等，方向相反。</p><p>FactoringRankNet先为给定query计算所有的权重更新，再应用这些更新，这种问题分解的方式加快了模型的训练。</p><h4 id="lambdarank">LambdaRank</h4><p>以上两种算法的优化方向都是减少排序错误的样本对的数量，平等地看待每个样本对，不能很好地匹配NDCG这种依赖位置信息的指标。LambdaRank的核心思想是直接调整在梯度方向上的步长，避免了从NDCG这种不可导的成本函数中推导梯度的困难。在训练模型的过程中，我们需要的不是成本函数的输出，需要的只有梯度。</p><p>我们重新定义成本函数<span class="math inline"><em>C</em></span>:<spanclass="math display"><em>C</em> = ∑<sub>{<em>i</em>, <em>j</em>} ∈ <em>I</em></sub>|<em>Δ</em><em>N</em><em>D</em><em>C</em><em>G</em><sub><em>i</em> ⇌ <em>j</em></sub>|log (1 + <em>e</em><sup>−<em>σ</em>(<em>s</em><sub><em>i</em></sub> − <em>s</em><sub><em>j</em></sub>)</sup>)</span>在前两种算法中<spanclass="math inline"><em>C</em></span>的定义的基础上增加一个乘数，表示仅交换结果列表中<spanclass="math inline"><em>i</em>, <em>j</em></span>的位置后NDCG的变化，相当于根据NDCG的变化量改变样本对的损失在成本函数中的权重。</p><p>我们定义在集合<spanclass="math inline"><em>I</em></span>种的样本对<spanclass="math inline">{<em>i</em>, <em>j</em>}</span>均满足<spanclass="math inline"><em>U</em><sub><em>i</em></sub> ▷ <em>U</em><sub><em>j</em></sub></span>，当模型正确地把<spanclass="math inline"><em>i</em></span>排在<spanclass="math inline"><em>j</em></span>前面时，<spanclass="math inline"><em>C</em></span>近似为0，当排序错误时，<spanclass="math inline"><em>C</em></span>近似正比于<spanclass="math inline">|<em>Δ</em><em>N</em><em>D</em><em>C</em><em>G</em><sub><em>i</em><em>j</em></sub>|<em>σ</em>(<em>s</em><sub><em>j</em></sub> − <em>s</em><sub><em>i</em></sub>)</span>。</p><p>重新计算<spanclass="math inline"><em>λ</em><sub><em>i</em><em>j</em></sub></span>：<span class="math display">$$\lambda_{ij} = \frac{\partial{C} } {\partial{s_i}} = \frac{-\sigma} {1 +e^{\sigma(s_i - s_j)} } \lvert \Delta NDCG \rvert\tag{14}$$</span> LambdaRank采用和FactoringRankNet相同的问题分解方式，在合并一个query中所有的样本对的贡献后再更新参数。</p><blockquote><p>论文<a href="#fn1" class="footnote-ref" id="fnref1"role="doc-noteref"><sup>1</sup></a>中提到改变<spanclass="math inline"><em>C</em></span>的定义后，模型的训练目标变成了最大化<spanclass="math inline"><em>C</em></span>，但是我认为优化目标依然是最小化<spanclass="math inline"><em>C</em></span>，并且原论文中存在前后矛盾的地方。</p><p>按照我的理解，则有： <span class="math display">$$\delta w_k = -\eta \frac{\partial{C} } {\partial{w_k} }$$</span></p><p><span class="math display">$$\delta s_i = \frac{\partial{s_i} } {\partial{w_k} } \delta{w_k}=  \frac{\partial{s_i} } {\partial{w_k} } (-\eta \lambda_i\frac{\partial{s_i} } {\partial{w_k} })$$</span></p><p><span class="math display">$$\delta s_j = \frac{\partial{s_j} } {\partial{w_k} } \delta{w_k}=  \frac{\partial{s_j} } {\partial{w_k} } (-\eta \lambda_j\frac{\partial{s_j} } {\partial{w_k} })$$</span></p><p>其中学习率<span class="math inline"><em>η</em> &gt; 0</span>，<spanclass="math inline"><em>λ</em><sub><em>i</em></sub> = ∑<sub><em>j</em> : {<em>i</em>, <em>j</em>} ∈ <em>I</em></sub><em>λ</em><sub><em>i</em><em>j</em></sub> − ∑<sub><em>j</em> : {<em>j</em>, <em>i</em>} ∈ <em>I</em></sub><em>λ</em><sub><em>i</em><em>j</em></sub></span>，因为<spanclass="math inline"><em>U</em><sub><em>i</em></sub> ▷ <em>U</em><sub><em>j</em></sub></span>，<spanclass="math inline"><em>λ</em><sub><em>i</em><em>j</em></sub> &lt; 0</span>，所以<spanclass="math inline"><em>λ</em><sub><em>i</em></sub> &lt; 0</span>，可以推导出：<spanclass="math display"><em>δ</em><em>s</em><sub><em>i</em></sub> &gt; 0</span></p><p><spanclass="math display"><em>δ</em><em>s</em><sub><em>j</em></sub> &lt; 0</span></p><p>模型对相关性更高的url <spanclass="math inline"><em>i</em></span>的打分增大，相关性低的url <spanclass="math inline"><em>j</em></span>的打分减小。</p></blockquote><h4 id="information-retrieval-measures">Information RetrievalMeasures</h4><h5 id="ndcg">NDCG</h5><section id="footnotes" class="footnotes footnotes-end-of-document"role="doc-endnotes"><hr /><ol><li id="fn1"><p><ahref="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf">MSR-TR-2010-82.pdf</a><ahref="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/06/07/hello-world/"/>
      <url>/2025/06/07/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> introduction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> introduction </tag>
            
            <tag> hello </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
