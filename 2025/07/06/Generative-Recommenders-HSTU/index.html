<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="本文整理分享了Meta在生成式推荐领域的开创性工作：通过借鉴LLM的思路，重塑推荐系统范式，实现推荐系统的scaling。传统的基于深度学习的推荐模型依赖高基数的，异构的特征，需要满足高吞吐，低时延的要求，每天处理百亿级别的用户操作。尽管经过大量数据（包含数千个特征）的训练，但大多数工业界的深度学习推荐模型（DLRMs）的效果仍无法随着计算能力的提升而扩展。受到LLM成功的启发，Meta团队重新审">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative Recommenders: HSTU">
<meta property="og:url" content="http://example.com/2025/07/06/Generative-Recommenders-HSTU/index.html">
<meta property="og:site_name" content="sherry-500">
<meta property="og:description" content="本文整理分享了Meta在生成式推荐领域的开创性工作：通过借鉴LLM的思路，重塑推荐系统范式，实现推荐系统的scaling。传统的基于深度学习的推荐模型依赖高基数的，异构的特征，需要满足高吞吐，低时延的要求，每天处理百亿级别的用户操作。尽管经过大量数据（包含数千个特征）的训练，但大多数工业界的深度学习推荐模型（DLRMs）的效果仍无法随着计算能力的提升而扩展。受到LLM成功的启发，Meta团队重新审">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/HSTU-1.png">
<meta property="og:image" content="http://example.com/images/HSTU-2.png">
<meta property="og:image" content="http://example.com/images/HSTU-3.png">
<meta property="og:image" content="http://example.com/images/HSTU-4.png">
<meta property="og:image" content="http://example.com/images/HSTU-5.png">
<meta property="og:image" content="http://example.com/images/HSTU-6.png">
<meta property="article:published_time" content="2025-07-06T14:12:15.000Z">
<meta property="article:modified_time" content="2025-07-08T12:59:34.735Z">
<meta property="article:author" content="sherry-500">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Recommendation System">
<meta property="article:tag" content="Information Retrieval">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/HSTU-1.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Generative Recommenders: HSTU</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">项目</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2025/06/16/Newton-Raphson-method/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2025/07/06/Generative-Recommenders-HSTU/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&text=Generative Recommenders: HSTU"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&is_video=false&description=Generative Recommenders: HSTU"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Generative Recommenders: HSTU&body=Check out this article: http://example.com/2025/07/06/Generative-Recommenders-HSTU/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&name=Generative Recommenders: HSTU&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&t=Generative Recommenders: HSTU"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#challenges"><span class="toc-number">1.</span> <span class="toc-text">Challenges</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#solution"><span class="toc-number">2.</span> <span class="toc-text">Solution</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%87%8D%E5%A1%91%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BC%8F%E6%8E%A8%E5%AF%BC%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.1.</span> <span class="toc-text">1.
将推荐系统重塑为序列式推导任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E5%BC%82%E6%9E%84%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1整合异构特征空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%A1%91%E5%8F%AC%E5%9B%9E%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BC%8F%E6%8E%A8%E5%AF%BC%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2
重塑召回和排序为序列式推导任务</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#retrieval"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Retrieval</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#rank"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Rank</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BDencoderhstu"><span class="toc-number">2.2.</span> <span class="toc-text">2.
为生成式推荐设计的高性能Encoder：HSTU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">3. 性能优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.1.</span> <span class="toc-text">3.1 流式训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E6%80%A7%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.2.</span> <span class="toc-text">3.2 稀疏性优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.3.</span> <span class="toc-text">3.3 内存优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%91%8A%E9%94%80"><span class="toc-number">2.3.4.</span> <span class="toc-text">3.4 计算摊销</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">3.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">4.</span> <span class="toc-text">References</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Generative Recommenders: HSTU
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">sherry-500</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-07-06T14:12:15.000Z" class="dt-published" itemprop="datePublished">2025-07-06</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a> › <a class="category-link" href="/categories/Deep-Learning/Recommendation-System/">Recommendation System</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a>, <a class="p-category" href="/tags/Information-Retrieval/" rel="tag">Information Retrieval</a>, <a class="p-category" href="/tags/LLM/" rel="tag">LLM</a>, <a class="p-category" href="/tags/Recommendation-System/" rel="tag">Recommendation System</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>本文整理分享了Meta在生成式推荐领域的开创性工作：通过借鉴LLM的思路，重塑推荐系统范式，实现推荐系统的scaling。传统的基于深度学习的推荐模型依赖高基数的，异构的特征，需要满足高吞吐，低时延的要求，每天处理百亿级别的用户操作。尽管经过大量数据（包含数千个特征）的训练，但大多数工业界的深度学习推荐模型（DLRMs）的效果仍无法随着计算能力的提升而扩展。受到LLM成功的启发，Meta团队重新审视了推荐系统的设计，将推荐问题重塑为序列式推导任务（sequential
transduction
tasks），提出了一种针对高基数，非静态的流式数据设计的新架构——HSTU。</p>
<h2 id="challenges">Challenges</h2>
<p>使用LLM的思路重塑传统推荐系统的范式主要面临三大挑战：</p>
<ol type="1">
<li><strong>异构特征</strong>：推荐系统中的特征缺乏显示的结构，异构特征（包括高基数id，交叉特征，计数，比率等）在推荐系统中发挥着重要的作用。</li>
<li><strong>动态大词表</strong>：不同于自然语言处理任务中使用的100K级别的静态词表，推荐系统需要使用动态变化的百亿级别的动态词表，从上万的候选物品中以目标已知的范式筛选，面临着巨大的训练和推理成本。</li>
<li><strong>计算成本生成式推荐在工业界落地的主要瓶颈</strong>：GPT-3使用上千个GPU在300B
tokens的训练数据上训练了1-2月，尽管这种规模已经很惊人，但是和推荐系统每天十亿级别的活跃用户相比简直就是毛毛雨。推荐系统每天需要处理的token数量就比语言模型1-2月内处理的token数量多几个数量级。</li>
</ol>
<p>为了解决上述挑战，Meta团队的贡献点总结如下：</p>
<ul>
<li>使用用户行为作为生成式建模中的一个新的模态，基于此，工业规模的推荐系统中的核心的召回和排序任务可以转化为生成建模问题，这一范式能够系统地利用特征，训练，推理中的冗余来提高效率。</li>
<li>首次提出Generative
Recommenders作为新的范式替代传统的DLRMs，序列化和整合异构的特征空间，随着序列长度趋向于无穷大，新方法能够近似整个DLRM的特征空间，这使得推荐系统中的主要问题，召回和排序可以转化为单纯的序列式推导任务，使模型能够以序列化的，生成式的方法被训练，进而可以用相同的计算量训练高一个数量级的数据。</li>
<li>提出了一种新的序列转换的结构，Hierarchy Sequential Transduction
Units（HSTU），HSTU针对动态大词表修改了注意力机制，使用一种新算法，M-FALCON，均摊计算成本。</li>
<li>通过新架构HSTU和训练算法GR，模型拥有1.5万亿个参数，模型总计算量达到1000x级的提升，第一次达到GPT-3
175b/LLaMa-2
70b等LLM训练算力，且第一次在推荐系统中观测到了类似的<strong>scaling
law</strong>。</li>
</ul>
<p><img src="/images/HSTU-1.png"></p>
<p>值得注意的是，GR的效果随着训练量的增长而增长，并且这种增长遵循一种幂率关系，这种关系在跨越了三个数量级的范围内成立，最高可达GPT-3或LLaMa-2这样的模型规模。</p>
<h2 id="solution">Solution</h2>
<p>Meta团队的这篇论文应该是最早从推荐系统的角度抽象通用设计范式的工作，在推荐系统问题的重塑上，主要面临几个核心洞察点。</p>
<h3 id="将推荐系统重塑为序列式推导任务">1.
将推荐系统重塑为序列式推导任务</h3>
<h4 id="整合异构特征空间">1.1整合异构特征空间</h4>
<p>目前流行的DLRM通常使用海量的异构特征训练，比如数值型特征和类别型特征。在GR中，我们需要将海量异构特征整合到一个统一的特征空间中，转换成适配生成式任务的输入格式——单一时间序列。</p>
<p><strong>类别型特征</strong>：比如用户喜欢的物品，用户的语言，用户加入的社区，都属于类别型特征。论文中采用的序列化方法是先从这些特征中选择时序最长的一种作为主时序，通常是用户交互过的物品的特征，其余特征的变动频率低，比如用户的人口统计特征，通过保留每个连续段的最早条目来压缩这些时序，然后将结果合并到主时序中，这种方法不会显著增加序列的长度。</p>
<p><strong>数值型特征</strong>：比如用户在特定类别的物品上的点击率，相比于类别型特征，这些特征变化更频繁，可能用户的每次交互都会造成变化，因此从计算和存储的角度看，完全序列化这类特征是不合理的。然而，在GR中，这类聚合统计特征用来聚合的分类特征已经被序列化和编码，如果将强大的序列推导结构（<strong>sequential
transduction
architecture</strong>）与目标感知（<strong>target-aware</strong>）的建模方式结合，也能够捕捉到用户兴趣。并且随着序列长度和计算量的增加，这种信息捕捉会越好，因此，可以直接移除数值统计特征。这也是典型的两个门派，长序列DNN端到端建模VS手动特征工程，海量样本/高频消费场景+长序列建模架构，是有可能更泛化地捕捉到用户兴趣，发挥特征工程的作用。但是在规模不足的场景下，手动特征工程仍然是必要的。</p>
<p><img src="/images/HSTU-2.png"></p>
<p>论文中没有具体说明tokenize的方式，并且论文的代码仓库从两个公开数据集构造的输入序列中也只包含物品id，没有辅助时序的具体实现以及物品侧除id以外的特征的处理。根据问题<a target="_blank" rel="noopener" href="https://github.com/meta-recsys/generative-recommenders/issues/28#issuecomment-2142681852">#28</a>的回答，可以用物品侧的特征生成token表示，比如用MLP作为一个最基础的实现。这种做法也解决了为不断产生的新物品生成token的问题。但还是有一个疑问：主时间序列和辅助时间序列的token如果是采用不同的方式生成的，如何将它们整合到统一的特征空间呢？</p>
<h4 id="重塑召回和排序为序列式推导任务">1.2
重塑召回和排序为序列式推导任务</h4>
<blockquote>
<p>这里先引入一个概念——<strong>transductive
learning</strong>，相对应的是<strong>inductive learning</strong>。</p>
<ul>
<li><strong>transductive
learning</strong>：在模型训练之初，就已经窥得训练集（带标签）和测试集（不带标签），尽管在训练时不知道测试集的真实标签，但可以从其特征分布中学到额外的信息，从而带来模型效果的增益，这也意味着，只要有新样本进来，模型就要重新训练。</li>
<li><strong>inductive
learning</strong>：传统的监督学习都可以理解为inductive
learning的范畴，基于训练集，训练模型，而后将其应用于测试集的预测任务中，测试集中的任何信息是没有在训练集中出现过的，即模型本身具备一定的通用性和泛化能力</li>
</ul>
</blockquote>
<p>给定按照时间顺序排列的token序列<span class="math inline"><em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, …, <em>x</em><sub><em>n</em> − 1</sub></span>和对应的观测时间序列<span class="math inline"><em>t</em><sub>0</sub>, <em>t</em><sub>1</sub>, …, <em>t</em><sub><em>n</em> − 1</sub></span>，sequential
transduction task将输入序列映射为输出序列<span class="math inline"><em>y</em><sub>0</sub>, <em>y</em><sub>1</sub>, …, <em>y</em><sub><em>n</em> − 1</sub></span>。</p>
<h5 id="retrieval">Retrieval</h5>
<p>在召回阶段，为用户学习概率分布<span class="math inline"><em>p</em>(<em>Φ</em><sub><em>i</em> + 1</sub>|<em>u</em><sub><em>i</em></sub>)</span>，其中<span class="math inline"><em>Φ</em><sub><em>i</em> + 1</sub> ∈ 𝕏<sub><em>c</em></sub></span>表示候选物品，<span class="math inline">𝕏<sub><em>c</em></sub> ⊆ 𝕏</span>，X是包含所有时序（item
id，人口统计特征，action等）的token的动态大词表，<span class="math inline"><em>u</em><sub><em>i</em></sub></span>是在时刻<span class="math inline"><em>t</em><sub><em>i</em></sub></span>的用户表征向量，从历史行为序列中学习到。学习目标是选择<span class="math inline"><em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>Φ</em> ∈ 𝕏<sub><em>c</em></sub></sub><em>p</em>(<em>Φ</em>|<em>u</em><sub><em>i</em></sub>)</span>，以最大化某种奖励。标准的自回归的设置与之不同：</p>
<ul>
<li>输入序列中同时包含正样本和负样本，所以<span class="math inline"><em>x</em><sub><em>i</em></sub></span>对应的监督<span class="math inline"><em>y</em><sub><em>i</em></sub></span>不一定是<span class="math inline"><em>Φ</em><sub><em>i</em> + 1</sub></span>，因为用户可能对<span class="math inline"><em>Φ</em><sub><em>i</em> + 1</sub></span>不感兴趣。</li>
<li>当<span class="math inline"><em>x</em><sub><em>i</em></sub></span>表示辅助时序的类别型特征时，<span class="math inline"><em>y</em><sub><em>i</em></sub></span>是未定义的</li>
</ul>
<p>GR采用自回归建模，decoder-only架构，以next token
prediction为训练任务，在计算损失时使用掩码机制，对下一个token用户反馈为负的位置不进行回归预测，但是在输入层面做self-attention时仍然用到了全部的行为反馈数据提取信息。虽然论文中没有详细说明损失函数是如何设计的，但是面对推荐系统海量的候选集，计算损失应该也引入了负采样优化。</p>
<h5 id="rank">Rank</h5>
<p>工业级的推荐系统的排序任务需要目标感知的建模，即目标与用户历史行为要尽可能早地交叉，但在标准的自回归设置中，这种交叉在encoder输出后接softmax层进行，无法在encoder内部底层做交叉，效果大打折扣。为了使得交叉能尽早进行，论文中采用的做法是改造输入序列，在主时序中交错排列item
token和action token，将排序任务形式化为<span class="math inline"><em>p</em>(<em>a</em><sub><em>i</em> + 1</sub>|<em>Φ</em><sub>0</sub>, <em>a</em><sub>0</sub>, <em>Φ</em><sub>1</sub>, <em>a</em><sub>1</sub>, …, <em>Φ</em><sub><em>i</em> + 1</sub>)</span>。在实际应用中，编码好的表征会接多个任务塔来实现多目标训练和预测。</p>
<p><img src="/images/HSTU-3.png"></p>
<p>这里论文依然没有详细说明损失函数是如何设计的，是否还需要自回归的损失，是只有多目标的损失，还是两者都有？</p>
<h3 id="为生成式推荐设计的高性能encoderhstu">2.
为生成式推荐设计的高性能Encoder：HSTU</h3>
<p>为了让生成式推荐扩展到具有海量动态词表的工业界大规模推荐系统中，Meta团队设计了一种新的Encoder架构，Hierarchical
Sequential Transduction Unit（HSTU）。</p>
<p>类似Transformer，HSTU堆叠多个层，层之间使用残差连接，每个层包含三个子层，分别是Pointwise投影层（Pointwise
Projection），空间聚合层（Spatial
Aggregation），Pointwise转换层（Pointwise Transformation）</p>
<ul>
<li><strong>Pointwise投影层</strong>：<span class="math inline"><em>U</em>(<em>X</em>), <em>V</em>(<em>X</em>), <em>Q</em>(<em>X</em>), <em>K</em>(<em>X</em>) = Split(<em>ϕ</em><sub>1</sub>(<em>f</em><sub>1</sub>(<em>X</em>)))</span></li>
<li><strong>Pointwise空间聚合层</strong>：<span class="math inline"><em>A</em>(<em>X</em>)<em>V</em>(<em>X</em>) = <em>ϕ</em><sub>2</sub>(<em>Q</em>(<em>X</em>)<em>K</em>(<em>X</em>)<sup><em>T</em></sup> + rab<sup><em>p</em>, <em>t</em></sup>)<em>V</em>(<em>X</em>)</span></li>
<li><strong>Pointwise转换层</strong>：<span class="math inline"><em>Y</em>(<em>X</em>) = <em>f</em><sub>2</sub>(Norm(<em>A</em>(<em>X</em>)<em>V</em>(<em>X</em>)) ⊙ <em>U</em>(<em>X</em>))</span></li>
</ul>
<p>其中，<span class="math inline"><em>f</em><sub>1</sub>, <em>f</em><sub>2</sub></span>是线性层，<span class="math inline"><em>ϕ</em></span>是非线性激活函数，采用SiLU，<span class="math inline">rab<sup><em>p</em>, <em>t</em></sup></span>是relative
attention bias，引入位置和时间信息，<span class="math inline">Norm</span>是Layer Normalization。</p>
<p><img src="/images/HSTU-4.png"></p>
<p>为了适应推荐系统的特性，HSTU对Transformer
Encoder做了以下几个方面的改进</p>
<ul>
<li><strong>Feature
Extraction</strong>：通过将item和action交替排列，HSTU能够以目标感知的方式，利用注意力机制获取池化的类别型特征，较早地进行特征交叉，这符合目前推荐系统领域中最先进的做法。</li>
<li><strong>Feature
Interaction</strong>：DLRM中通常采用FM，以及神经网络的变种（比如DCN）实现低阶，高阶的特征交叉。HSTU在传统Q，K，V的基础上增加了门控权重U，保留了底层的用户长期行为序列表征，采用attention计算后的pooled信息本身包含了和target的高阶交叉特征，HSTU通过<span class="math inline">Norm(<em>A</em>(<em>X</em>)<em>V</em>(<em>X</em>)) ⊙ <em>U</em>(<em>X</em>)</span>再补充与底层原始信息之间的特征交叉。</li>
<li><strong>Transformation of
Representation</strong>：传统DLRM通常基于MoE来动态路由选取多样异构的输入信息，一种想法是为每个用户专门定制子网络进行条件计算。在HSTU中，element-wise点积可以在标准化因子上实现类似MoE的门控操作。<span class="math inline">Norm(<em>A</em>(<em>X</em>)<em>V</em>(<em>X</em>))</span>可以类比MoE中的门控操作，对底层<span class="math inline"><em>U</em>(<em>X</em>)</span>做信息筛选。</li>
<li><strong>Pointwise Aggregation
Attention</strong>：HSTU采用一种新的Pointwise Aggregation
Attention机制替代softmax
attention，抛弃了使用softmax做归一化的操作，能够更好地保留用户兴趣的强度。推荐系统不仅需要预测item之间的相对顺序，还需要预测具体的值，比如用户花在特定物品上的时长。如果采用softmax做归一化，虽然序信息可以保留，但是与目标相关的先验数据点的数量是一个能表明用户偏好强度的强有力的特征，在经过softmax归一化后很难捕捉。比如，用户的历史行为中包含90%的衣服，10%的电子产品，对于目标的电子产品的候选item，显然行为序列中为电子产品的item的权重接近0，目标为衣服，softmax也会放大权重，都会导致偏好强度信息失真。除此以外，作者通过生成数据模拟流式环境，发现softmax不适合动态词表的场景。</li>
<li><strong>removal of
FFN</strong>：在推荐系统中，出于对训练通量和模型效果的考虑，使用较大的batch
size是必要的。相比于LLM通常使用较小的batch
size训练，内存占用由模型参数主导，GR中激活的内存占用是扩展的主要瓶颈。HSTU移除了Transfomer中的前馈层，将attention外部的线性层数量从六个减少为两个。为了弥补移除FFN带来的非线性表达能力减弱的问题，HSTU在投影层中引入了非线性激活函数。</li>
</ul>
<h3 id="性能优化">3. 性能优化</h3>
<p>性能是生成式推荐在工业级推荐系统中落地的最主要瓶颈，论文中介绍了多种优化手段。</p>
<h4 id="流式训练">3.1 流式训练</h4>
<p>工业级的推荐系统通常在流式数据上训练，每个样本曝光后会被顺序地处理。在这种设置下，基于self-attention的序列式直推架构，如Transformer的复杂度为<span class="math inline">∑<sub><em>i</em></sub><em>n</em><sub><em>i</em></sub>(<em>n</em><sub><em>i</em></sub><sup>2</sup><em>d</em> + <em>n</em><sub><em>i</em></sub><em>d</em><sub><em>f</em><em>f</em></sub><em>d</em>)</span>。最外层的<span class="math inline"><em>n</em><sub><em>i</em></sub></span>表示待训练的每个token都要进行一遍和用户行为序列的交叉。这种时间复杂度在实际应用中是无法接受的。</p>
<p>为了解决序列式直推模型在长序列上训练的可扩展性问题，使用生成式训练（generative
training）替代传统的曝光级别的训练范式（impression-level
training），可以将时间复杂度降低一个<span class="math inline"><em>O</em>(<em>N</em>)</span>的因子。一种具体的实现方法是在用户的一次请求或者会话结束时，释放训练样本，这样encoder部分的计算成本可以在多个target中均摊。</p>
<h4 id="稀疏性优化">3.2 稀疏性优化</h4>
<p>推荐系统中，用户历史行为序列的长度通常服从偏斜分布，导致输入序列稀疏，尤其是在序列长度设置得较长时。可以利用这种稀疏性来显著提高encoder的效率。为了实现这一点，作者开发了一种高效的GPU注意力kernel，类似于FlashAttention，将注意力计算转化为不同大小的分组GEMM计算。因此，HSTU中的自注意力计算以内存为bound，按照<span class="math inline"><em>Θ</em>(∑<sub><em>i</em></sub><em>n</em><sub><em>i</em></sub><sup>2</sup><em>d</em><sub><em>q</em><em>k</em></sub><sup>2</sup><em>R</em><sup>−1</sup>)</span>来scaling，其中<span class="math inline"><em>R</em></span>是寄存器大小。这种方法能带来2-5倍的吞吐量提升。</p>
<p>除了推理的稀疏性优化，也可以在算法层面做一些稀疏性约束。通过随机序列长度（Stochastic
Length）增加用户历史序列的稀疏性。推荐系统中用户的行为序列往往在时间上具有重复性，并以多种形式呈现，因此比较适合在算法侧注入稀疏性，且不会降低模型的质量。SL的工作原理如下：</p>
<ul>
<li><span class="math inline">(<em>x</em><sub><em>i</em></sub>)<sub><em>i</em> = 0</sub><sup><em>n</em><sub><em>c</em>, <em>j</em></sub></sup>
<em>i</em><em>f</em><em>n</em><sub><em>c</em>, <em>j</em></sub> ≤ <em>N</em><sub><em>c</em></sub><sup><em>α</em>/2</sup></span>，其中<span class="math inline"><em>N</em><sub><em>c</em></sub></span>是样本中最大的序列长度，<span class="math inline"><em>n</em><sub><em>c</em>, <em>j</em></sub></span>是用户j的历史交互物品序列的长度</li>
<li><span class="math inline"><em>i</em><em>f</em><em>n</em><sub><em>c</em>, <em>j</em></sub> &gt; <em>N</em><sub><em>c</em></sub><sup><em>α</em>/2</sup></span>，以<span class="math inline"><em>N</em><sub><em>c</em></sub><sup><em>α</em></sup>/<em>n</em><sub><em>c</em>, <em>j</em></sub><sup>2</sup></span>的概率保留原序列，以<span class="math inline">1 − <em>N</em><sub><em>c</em></sub><sup><em>α</em></sup>/<em>n</em><sub><em>c</em>, <em>j</em></sub><sup>2</sup></span>的概率从原始序列中采样出子序列：<span class="math inline">(<em>x</em><sub><em>i</em><sub><em>k</em></sub><sub><em>k</em> = 0</sub><sup><em>N</em><sub><em>c</em></sub><sup><em>α</em>/2</sup></sup></sub>)</span></li>
</ul>
<p>这种方法将与attention相关的事件复杂度减小到<span class="math inline"><em>O</em>(<em>N</em><sup><em>α</em></sup><em>d</em>)</span>，其中<span class="math inline"><em>α</em> ∈ (1, 2]</span>。引入SL机制能降低训练过程的计算开销，相比于推理，训练的计算成本更大。有以下观察：</p>
<ul>
<li>当<span class="math inline"><em>α</em> = 2</span>时，表示没有使用使用SL</li>
<li>较小的<span class="math inline"><em>α</em></span>适用于更长的序列</li>
</ul>
<p><img src="/images/HSTU-5.png"></p>
<h4 id="内存优化">3.3 内存优化</h4>
<p>前文中，我们已经提到在推荐系统中应用序列式直推架构的内存占用是由激活主导的，为了优化内存占用，HSTU采用了简化且完全融合的设计（simplified
and fully fused
design），将线性层的数量从6个减少到2个。激活的内存占用减少到每层<span class="math inline">2<em>d</em> + 2<em>d</em> + 4<em>h</em><em>d</em><sub><em>q</em>, <em>k</em></sub> + 4<em>h</em><em>d</em><sub><em>v</em></sub> + 2<em>h</em><em>d</em><sub><em>v</em></sub> = 14<em>d</em></span>。</p>
<p>为了进行比较，Transformer在注意力之后使用一个前馈层和注意力之间的dropout（<span class="math inline">3<em>h</em><em>d</em><sub><em>v</em></sub></span>的中间状态），接着是一个由层归一化，线性层，激活函数，线性层和dropout组成的前馈层，中间状态为（<span class="math inline">2<em>d</em> + 4<em>d</em><sub><em>f</em><em>f</em></sub> + 2<em>d</em> + 1<em>d</em> = 4<em>d</em> + 4<em>d</em><sub><em>f</em><em>f</em></sub></span>），我们假设<span class="math inline"><em>d</em><sub><em>f</em><em>f</em></sub> = 4<em>d</em></span>，则激活占用的内存达到<span class="math inline">33<em>d</em></span>。因此HSTU的设计省下的内存使其网络层数能扩展到&gt;2倍的更深层次。</p>
<h4 id="计算摊销">3.4 计算摊销</h4>
<p>最后一个挑战是推荐系统线上服务时需要推理大量的候选物品。在召回阶段encoder的计算时可以完全摊销的，因为GR会根据用户的历史序列预测下一个token，采用向量召回的方法筛选出候选物品。而在排序阶段，面临着数以万计的候选item，论文中提出了一种名为M-FALCON（Microbatched-Fast
Attention Leveraging Cacheable
OperationNs）的算法，用于对m个候选项同时推理。</p>
<p>在前向传播中，M-FALCON通过修改注意力掩码和偏置<span class="math inline"><em>r</em><em>a</em><em>b</em><sup><em>p</em>, <em>t</em></sup></span>，使得<span class="math inline"><em>b</em><sub><em>m</em></sub></span>个候选物品的注意力操作完全相同，从而实现并行推理。crosss-attention的成本可以从<span class="math inline"><em>O</em>(<em>b</em><sub><em>m</em></sub><em>n</em><sup>2</sup><em>d</em>)</span>降低到<span class="math inline"><em>O</em>((<em>n</em> + <em>b</em><sub><em>m</em></sub>)<sup>2</sup><em>d</em>) = <em>O</em>(<em>n</em><sup>2</sup><em>d</em>)</span>，也就是将目标item的计算融入到长度为<span class="math inline"><em>n</em> + <em>b</em><sub><em>m</em></sub></span>的序列的计算过程中，同时推理多个item的打分。将候选物品分成大小为<span class="math inline"><em>b</em><sub><em>m</em></sub></span>的批次，以利用<strong>encoder级别的KV缓存</strong>来实现请求之间的计算复用，降低计算成本。比如<span class="math inline"><em>K</em>(<em>X</em>), <em>V</em>(<em>X</em>)</span>可以缓存起来，以便在请求内或请求间的批次之间复用。利用缓存之后，在一次前向传播的过程中，只需要计算序列最后<span class="math inline"><em>b</em><sub><em>m</em></sub></span>个token对应的<span class="math inline"><em>U</em>(<em>X</em>), <em>Q</em>(<em>X</em>), <em>K</em>(<em>X</em>), <em>V</em>(<em>X</em>)</span>，同样在其他的计算步骤中，也只需要考虑<span class="math inline"><em>b</em><sub><em>m</em></sub></span>个候选物品，前向传播的时间复杂度从<span class="math inline"><em>O</em>((<em>n</em> + <em>b</em><sub><em>m</em></sub>)<em>d</em><sup>2</sup> + (<em>n</em> + <em>b</em><sub><em>m</em></sub>)<sup>2</sup><em>d</em>)</span>减小到<span class="math inline"><em>O</em>(<em>b</em><sub><em>m</em></sub><em>d</em><sup>2</sup> + <em>b</em><sub><em>m</em></sub><em>n</em><em>d</em>)</span>。这种程度的减少是非常可观的。</p>
<p><img src="/images/HSTU-6.png"></p>
<h2 id="conclusion">Conclusion</h2>
<p>这篇论文将推荐系统中核心的召回和排序问题重塑为序列式直推任务，提出了一种新的范式Generative
Recommenders，成功在推荐系统中落地生成式训练。</p>
<ul>
<li>通过减少对大量异构特征的依赖，可以使推荐系统在改善用户体验的同时更注重隐私保护</li>
<li>长序列建模一直是推荐系统领域优化的重点方向之一，生成式推荐通过使用全序列建模的方法，可以考虑到长期目标对短期决策的影响。将用户的长期目标纳入推荐算法中，能更准确地为用户提供符合用户长期目标的内容，减少传播点击诱导和虚假内容，更好地对齐平台激励和用户价值。</li>
</ul>
<p>其实这篇论文还是挺难理解的，Meta团队的工作非常具有开创性和启发性，论文中的许多设计和优化方法需要对推荐系统和LLM两个领域都有比较深入的了解，才能理解背后的巧思。同时，一些技术细节论文中并没有交代清楚，比如召回和排序阶段的GR是否共享，还是分开的两个模型？总体来说，这篇论文的工作融合了传统推荐系统分层架构和算法设计特点，LLM中Tansformer
scaling优势，既没有完全颠覆传统的推荐系统范式，也不是只在传统范式上引入LLM能力的小改进。首次展示了从LLM中得到的scaling
law同样适用于大规模推荐系统，并成功落地。</p>
<h2 id="references">References</h2>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/tags/">标签</a></li>
        
          <li><a href="/categories/">分类</a></li>
        
          <li><a href="/search/">搜索</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">项目</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#challenges"><span class="toc-number">1.</span> <span class="toc-text">Challenges</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#solution"><span class="toc-number">2.</span> <span class="toc-text">Solution</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%87%8D%E5%A1%91%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BC%8F%E6%8E%A8%E5%AF%BC%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.1.</span> <span class="toc-text">1.
将推荐系统重塑为序列式推导任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E5%BC%82%E6%9E%84%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1整合异构特征空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%A1%91%E5%8F%AC%E5%9B%9E%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BC%8F%E6%8E%A8%E5%AF%BC%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2
重塑召回和排序为序列式推导任务</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#retrieval"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Retrieval</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#rank"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Rank</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E8%8D%90%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BDencoderhstu"><span class="toc-number">2.2.</span> <span class="toc-text">2.
为生成式推荐设计的高性能Encoder：HSTU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">3. 性能优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.1.</span> <span class="toc-text">3.1 流式训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E6%80%A7%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.2.</span> <span class="toc-text">3.2 稀疏性优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.3.</span> <span class="toc-text">3.3 内存优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%91%8A%E9%94%80"><span class="toc-number">2.3.4.</span> <span class="toc-text">3.4 计算摊销</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">3.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">4.</span> <span class="toc-text">References</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2025/07/06/Generative-Recommenders-HSTU/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&text=Generative Recommenders: HSTU"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&is_video=false&description=Generative Recommenders: HSTU"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Generative Recommenders: HSTU&body=Check out this article: http://example.com/2025/07/06/Generative-Recommenders-HSTU/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&title=Generative Recommenders: HSTU"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&name=Generative Recommenders: HSTU&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2025/07/06/Generative-Recommenders-HSTU/&t=Generative Recommenders: HSTU"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2025
    sherry-500
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">项目</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'sherry-500/sherry-500.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'Comment';
      var utterances_theme = 'github-light';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
